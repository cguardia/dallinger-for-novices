The Database
============

At the heart of Dallinger is the database. If you've already installed Dallinger (thumbs up) you might remember having to install something called "Postgres". That's a program that allows you to create databases on your own machine, and so is essential for running Dallinger locally (i.e. in "debug mode"). You might also have installed a program called "Postico". This is a database viewer, allowing you to open up the database and look inside.

Let's do this now. First, open up the terminal and navigate to the directory containing the Bartlett1932 demo (its address is Dallinger/demos/dlgr/demos/bartlett1932). Once you're there, execute the command ``dallinger debug`` and the experiment will run locally on your machine (remember you can use the command ``dallinger debug --verbose`` to see more information as the experiment runs). After a few seconds of thinking, dallinger should pop open a new browser window inviting you to take part in the study. You can take part now, but instead, let's look at the database in Postico. Once you've opened the database you should see that it consists of a series of tables with names like `info` and `vector`.

You can double-click on these tables to view their contents. Right now almost all of them are empty, though a couple will have some entries in them. You don't need to understand what the entries (or lack thereof) mean right now. Simply note that from this point of view, the database looks just like a series of spreadsheets: there are columns with names, and entries are stored as rows in the table. Morevoer, there are different tables for different kinds of things: a table for Nodes, a table for Vectors and so on. In fact, there's a separate table for almost all of the key classes used in Dallinger. It is perfectly fine to think of the database in this way --- as a series of tables --- but this ignores some of its most powerful and useful features, so we'll briefly discuss the dual nature of the database now.

The Dual Nature of Dallingers Things
------------------------------------

I'm going to start this section with a digression to get you started thinking about how single entities can be dual natured. If you did high school physics this might already be reasonably familiar to you. If not, I hope it is atleast comprehensible and maybe even useful!

OK, so back in the 19th century, scientists had noted the general pattern that things were either waves or particles. Examples of waves are things like sound which exists as patterns of vibrations in a substance (e.g. air on Earth). Critically, waves cannot exist without their substrate, so sound cannot pass through a vacuum. On the other hand there are things that are made of particles. For instance, a piece of iron is made of iron atoms, which are themselves made of protons, neutrons and electons. These things do not require a medium in which to exist, and they can be counted individually.

Light, however, stubbornly refused to fit neatly into either category. On the one hand it could refract and diffuse, key behaviors of waves. On the other hand, no known medium for light was known (it happily passes through a vacuum). Things came to a head when Einstein documented the "photoelectric effect". Basically light could be shone on a material and the energy of the light would cause electrons to be released. The key was to vary the amount of energy in the light. If light were a wave, even low energy light would eventually ping off some electrons, it would just take a while for enough energy to arrive. Instead the opposite pattern was seen; there was a threshold energy below which electrons never escaped, but above which electrons were immediately released. This is clear particle behavior; either the light particles had enough energy per particle to release electrons, or they didn't in which case they harmlessly bounced off their targets. So now we have particles of light that nonetheless can diffuse, diffract and refract like a wave. This introduced the concept of wave/particle duality. The idea being that at the smallest scales, things are neither wave nor particle, but some kind of hybrid that can behave like both. Confirming this, further experiments were able to get things like electrons, that had hitherto seemed to be clear examples of particles, to diffuse and diffract, showing that they too exhibited wave/particle duality.

OK, so back to Dallinger. In the introduction above we saw how, from one view, the various objects that make up a Dallinger experiment (Nodes, Vectors and so on) look just like rows in a table, with a different table for each kind of object. This is what you see with your database viewing program (e.g. Postico). Because of this, anything you can do with a row of data you can do with nodes. For instance you can look them up by a particular value, or order them by a particular column (not the most exciting stuff I admit). But, when we create a Dallinger experiment these different kinds of objects will take on a second aspect and they will have properties and abilities that seem to go beyond what a simple row of data can do. Instead they'll feel more like an actual object that you can interact with. Compare a real life kettle with a listing of a kettle on a shopping website. From the website you can read all sorts of things about the kettle, but you can't get it to boil some water, for that you need an actual instance of the kettle to be delivered to you. It's the same with Dallinger's objects except the objects seem to have the best of both worlds: the database is like the shopping website, you can see all sorts of things about (e.g.) Nodes, but nonetheless you can still tell a particular node to do something, like connect to another node and send it some information. This is the dual nature of Dallinger's objects - they are both a row-in-a-table and an object, seemingly at the same time.

Let's illustrate this with an example. Imagine you want to know the creation time of the network that contains the node with id 5. This might sound a little daunting, but it you dig around in your database viewer you'll see that the node table contains a column called `network_id` and the network table contains a column called `creation_time`. If we approach this problem thinking in terms of tables, here's a rough description of how we could solve it:

1. Do a query over the node table asking for all rows with an id of 5 (there will only be one)
2. Look at the `network_id` of the returned row; let's say it's 3
3. Do a query over the network table asking for all rows with an id of 3 (again, there will only be one)
4. Look at the `creation_time` of the returned row

This will work, but it's a little clunky. Once you get more comfortable with approaching rows in the tables as if they were smart objects, you can do something more like this:

1. Do a query over the node table asking for all rows with an id of 5 (there will only be one).
2. Ask the node for its network.
3. Ask the network for its creation time.

In fact, if your back-end experiment code were to involve this request, it would look remarkably clean. Let's assume you've looked up the node with an id of 5 and you've stored it as a variable called ``node5``. To get the creation time of its network, you can simply write ``node5.network.creation_time`` and this will give you the exact time at which that network was created (i.e. the exact time that the row corresponding to network 3 was entered into the network table). Lovely!

So from this I hope you can see that the kinds of objects that Dallinger creates have a kind of row-in-a-table/object duality, just like subatomic particles can have wave/particle duality. Now, one big difference is that wave/particle duality points at some deep meaningful truth about the universe, while Dallinger's row-in-a-table/object duality doesn't. In fact this kind of duality is a deliberately engineered feature. Things created by Dallinger `have to` be stored in the database, but often working directly with the database is clunky and so we typically work with them in "object mode". This transformation is handled by the libraries that Dallinger is built upon, so you don't need to worry about the deails of it, but basically as soon as you ask for anything from the database, rather than being given to you as a row (or set of rows), it will be given to you as an object (or a list of objects for multiple rows).

Why do we have a database?
--------------------------

Given the above, where a problem can be solved quickly and elegantly by thinking in terms of objects, but slowly and clunkily by thinking in terms of rows in tables, you might ask why we have tables at all. Can't we just have a database of objects?

One reason for the database is that, at the end of your experiment, you'll somehow want to export all the data generated by your experiment for analysis and so on. The database is what gets exported. Basically, all the tables are converted into csv files, then dumped into a folder inside your experiment directory. Obviously at this point the row/object duality is broken: you now really do just have a bunch of tables. This might not seem totally satisfactory though, after all, why not have the experiment use objects and just create the tables for when you need to export the data at the end?

Here's why this wouldn't work. Imagine you are running an experiment and 50 participants are taking part at the same time. They are all clicking away on their computers and this is sending various requests to the server to record their responses and advance the experiment, and so on. However, each request takes time, and even though each request might only require a few milliseconds there will come a point at which the server can't process the requests fast enough. This will lead to a growing backlog of requests and eventually the server will crash and you'll get a bunch of emails from annoyed participants. The solution is to parallelize the server - in effect we have multiple servers (called `threads`) which run in parallel to get through requests far faster than they come in. But this raises a problem; if there are now multiple threads how can we be sure that they agree on the current state of the experiment? Without something to hold them together they might drift apart and so our experiment will become a total mess. What we need is some `separate process` to store the "truth" of the experiment.

To solve this, let's make one more thread that doesn't handle requests, but rather stores in its memory all of the experiment as objects. Now, when the other threads are handling requests they check in with this thread to confirm the current state of the experiment. But now we are back to the same problem we started with: because all requests need to go through this "master" thread, we have a major bottleneck and in effect the experiment will be no faster than if it had a single thread. The solution is to replace the master thread with a set of tables and all the other threads to examine and edit the tables at the same time. In this way the database stores the "truth" as rows in a table and the server threads read from the database, turning the rows in the database into objects in their memory as necessary.

You might be worried that two threads might try to edit the same row at the same time (and this would indeed be a problem), but fortunately there's an easy solution: While a thread is reading/editing specific rows in the database, those rows can be `locked` and any other threads that want to read or edit them have to wait. Because locking can be done on a row-by-row basis, the rest of the database remains totally accessible (this locking is also done automatically so you don't need to worry about it). Moreover, because multiple threads only rarely try to access the same rows this causes only a tiny slow down in the rate at which the server can process.

So, even though it's nicer to think in terms of objects, we need a database with tables and rows because otherwise we are forced to have only a single server thread handling all incoming requests which is too slow for anything other than the smallest experiment.
